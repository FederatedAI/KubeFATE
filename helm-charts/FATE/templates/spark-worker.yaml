# Copyright 2019-2020 VMware, Inc.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

{{ if .Values.modules.spark.include }}
kind: ConfigMap
apiVersion: v1
metadata:
  name: spark-worker-config
  labels:
    fateMoudle: spark-worker
{{ include "fate.labels" . | indent 4 }}
data:
  service_conf.yaml: |
    use_registry: false
    use_deserialize_safe_module: false
    dependent_distribution: false
    fateflow:
      # you must set real ip address, 127.0.0.1 and 0.0.0.0 is not supported
      host: 127.0.0.1
      http_port: 9380
      grpc_port: 9360
      http_app_key:
      http_secret_key:
      # support rollsite/nginx/fateflow as a coordination proxy
      # rollsite support fate on eggroll, use grpc protocol
      # nginx support fate on eggroll and fate on spark, use http or grpc protocol, default is http
      # fateflow support fate on eggroll and fate on spark, use http protocol, but not support exchange network mode
        
      # format(proxy: rollsite) means rollsite use the rollsite configuration of fate_one_eggroll and nginx use the nginx configuration of fate_one_spark
      # you also can customize the config like this(set fateflow of the opposite party as proxy):
      # proxy:
      #   name: fateflow
      #   host: xx
      #   http_port: xx
      #   grpc_port: xx
      proxy: rollsite
      # support default/http/grpc
      protocol: default
    database:
      name: fate_flow
      user: fate
      passwd: fate
      host: 127.0.0.1
      port: 3306
      max_connections: 100
      stale_timeout: 30
    zookeeper:
      hosts:
        - 127.0.0.1:2181
      use_acl: false
      user: fate
      password: fate
    # engine services
    default_engines:
      computing: standalone
      federation: standalone
      storage: standalone
    fate_on_standalone:
      standalone:
        cores_per_node: 20
        nodes: 1
    fate_on_eggroll:
      clustermanager:
        cores_per_node: 16
        nodes: 1
      rollsite:
        host: 127.0.0.1
        port: 9370
    fate_on_spark:
      spark:
        # default use SPARK_HOME environment variable
        home:
        cores_per_node: 20
        nodes: 2
      linkis_spark:
        cores_per_node: 20
        nodes: 2
        host: 127.0.0.1
        port: 9001
        token_code: MLSS
        python_path: /data/projects/fate/python
      hive:
        host: 127.0.0.1
        port: 10000
        auth_mechanism:
        username:
        password:
      linkis_hive:
        host: 127.0.0.1
        port: 9001
      hdfs:
        name_node: hdfs://fate-cluster
        # default /
        path_prefix:
      rabbitmq:
        host: 192.168.0.4
        mng_port: 12345
        port: 5672
        user: fate
        password: fate
        # default conf/rabbitmq_route_table.yaml
        route_table:
      pulsar:
        host: 192.168.0.5
        port: 6650
        mng_port: 8080
        cluster: standalone
        # all parties should use a same tenant
        tenant: fl-tenant
        # message ttl in minutes
        topic_ttl: 5
        # default conf/pulsar_route_table.yaml
        route_table:
      nginx:
        host: 127.0.0.1
        http_port: 9300
        grpc_port: 9310
    # external services
    fateboard:
      host: 127.0.0.1
      port: 8080
    
    # on API `/model/load` and `/model/load/do`
    # automatic upload models to the model store if it exists locally but does not exist in the model storage
    # or download models from the model store if it does not exist locally but exists in the model storage
    # this config will not affect API `/model/store` or `/model/restore`
    enable_model_store: false
    model_store_address:
      # use mysql as the model store engine
    #  storage: mysql
    #  database: fate_model
    #  user: fate
    #  password: fate
    #  host: 127.0.0.1
    #  port: 3306
      # other optional configs send to the engine
    #  max_connections: 10
    #  stale_timeout: 10
      # use redis as the model store engine
    #  storage: redis
    #  host: 127.0.0.1
    #  port: 6379
    #  db: 0
    #  password:
      # the expiry time of keys, in seconds. defaults None (no expiry time)
    #  ex:
      # use tencent cos as model store engine
      storage: tencent_cos
      Region:
      SecretId:
      SecretKey:
      Bucket:
    
    servings:
      hosts:
        - 127.0.0.1:8000
    fatemanager:
      host: 127.0.0.1
      port: 8001
      federatedId: 0

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-worker
  labels:
    fateMoudle: spark-worker
{{ include "fate.labels" . | indent 4 }}
spec:
  replicas: {{ default 2 .Values.modules.spark.worker.replicas }}
  strategy:
    type: Recreate
  selector:
    matchLabels:
      fateMoudle: spark-worker
{{ include "fate.matchLabels" . | indent 6 }}
  template:
    metadata:
      labels:
        fateMoudle: spark-worker
{{ include "fate.labels" . | indent 8 }}
    spec:
      containers:
        - name: spark-worker
          image: {{ if .Values.modules.spark.worker.Image }}{{ .Values.modules.spark.worker.Image }}{{ else }}{{ .Values.image.registry }}/spark-worker{{ end }}:{{ default .Values.image.tag .Values.modules.spark.worker.ImageTag }}
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          {{- if .Values.modules.spark.worker.resources }}
          resources:
          {{- range $key, $val := .Values.modules.spark.worker.resources }}
            {{ $key }}:
{{ toYaml $val | indent 14 }}
          {{- end }}
          {{- end }}
          volumeMounts:
            - mountPath: /data/projects/fate/conf/
              name: spark-worker-confs
          ports:
            - containerPort: 8081
      {{- with .Values.modules.spark.worker.nodeSelector }}
      nodeSelector: 
{{ toYaml . | indent 8 }}
      {{- end }}
      {{- with .Values.modules.spark.worker.tolerations }}
      tolerations:
{{ toYaml . | indent 8 }}
      {{- end }}
      {{- with .Values.modules.spark.worker.affinity }}
      affinity:
{{ toYaml . | indent 8 }}
      {{- end }}
      {{- with .Values.image.imagePullSecrets }}
      imagePullSecrets:
{{ toYaml . | indent 6 }}
      {{- end }}
      serviceAccountName: {{ template "serviceAccountName" . }}
      restartPolicy: Always
      volumes:
        - name: spark-worker-confs
          configMap:
            name: spark-worker-config
---
apiVersion: v1
kind: Service
metadata:
  name: spark-worker-1
  labels:
    fateMoudle: spark-worker
{{ include "fate.labels" . | indent 4 }}
spec:
  ports:
    - name: "tcp-spark"
      port: 8081
      targetPort: 8081
      protocol: TCP
  type: {{ .Values.modules.spark.worker.type }}
  clusterIP: None
  selector:
    fateMoudle: spark-worker
{{ include "fate.matchLabels" . | indent 4 }}
---
{{ end }}
