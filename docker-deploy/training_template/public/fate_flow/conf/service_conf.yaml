use_registry: false
use_deserialize_safe_module: false
dependent_distribution: false
fateflow:
  # you must set real ip address, 127.0.0.1 and 0.0.0.0 is not supported
  host: 192.167.0.100
  http_port: 9380
  grpc_port: 9360
  # http_app_key:
  # http_secret_key:
  # support rollsite/nginx/fateflow as a coordination proxy
  # rollsite support fate on eggroll, use grpc protocol
  # nginx support fate on eggroll and fate on spark, use http or grpc protocol, default is http
  # fateflow support fate on eggroll and fate on spark, use http protocol, but not support exchange network mode

  # format(proxy: rollsite) means rollsite use the rollsite configuration of fate_one_eggroll and nginx use the nginx configuration of fate_one_spark
  # you also can customize the config like this(set fateflow of the opposite party as proxy):
  # proxy:
  #   name: fateflow
  #   host: xx
  #   http_port: xx
  #   grpc_port: xx
  proxy: rollsite 
  # support default/http/grpc
  protocol: default
  # It can also be specified in the job configuration using the federated_status_collect_type parameter
  default_federated_status_collect_type: PULL
database:
  name: <db_name>
  user: <db_user>
  passwd: <db_passwd>
  host: <db_host>
  port: 3306
  max_connections: 100
  stale_timeout: 30
zookeeper:
  hosts:
  - "serving-zookeeper:2181"
  # use_acl: false
  # user: fate
  # password: fate
# # engine services
default_engines:
  computing: eggroll
  federation: eggroll
  storage: eggroll
fate_on_standalone:
  standalone:
    cores_per_node: 20
    nodes: 1
fate_on_eggroll:
  clustermanager:
    cores_per_node: 16
    nodes: 1
  rollsite:
    host: rollsite 
    port: 9370
fate_on_spark:
  spark:
    # default use SPARK_HOME environment variable
    home: /data/projects/spark-2.4.1-bin-hadoop2.7/
    cores_per_node: 20
    nodes: 2
  linkis_spark:
    cores_per_node: 20
    nodes: 2
    host: 127.0.0.1
    port: 9001
    token_code: MLSS
    python_path: /data/projects/fate/python
  hive:
    host: 127.0.0.1
    port: 10000
    auth_mechanism:
    username:
    password:
  linkis_hive:
    host: 127.0.0.1
    port: 9001
  hdfs:
    name_node: hdfs://namenode:9000
    # default /
    path_prefix:
  rabbitmq:
    host: rabbitmq 
    mng_port: 15672 
    port: 5672
    user: fate
    password: fate
    # default conf/rabbitmq_route_table.yaml
    route_table:
  pulsar:
    host: pulsar
    mng_port: 8080
    port: 6650
    cluster: standalone
    # all parties should use a same tenant
    tenant: fl-tenant
    # message ttl in minutes
    topic_ttl: 5
    # default conf/pulsar_route_table.yaml
    route_table:
  nginx:
    host: nginx 
    http_port: 9300
    grpc_port: 9310
# external services
fateboard:
  host: fateboard
  port: 8080

# on API `/model/load` and `/model/load/do`
# automatic upload models to the model store if it exists locally but does not exist in the model storage
# or download models from the model store if it does not exist locally but exists in the model storage
# this config will not affect API `/model/store` or `/model/restore`
enable_model_store: false
model_store_address:
  # use mysql as the model store engine
#  storage: mysql
#  database: fate_model
#  user: fate
#  password: fate
#  host: 127.0.0.1
#  port: 3306
  # other optional configs send to the engine
#  max_connections: 10
#  stale_timeout: 10
  # use redis as the model store engine
#  storage: redis
#  host: 127.0.0.1
#  port: 6379
#  db: 0
#  password:
  # the expiry time of keys, in seconds. defaults None (no expiry time)
#  ex:
  # use tencent cos as model store engine
  storage: tencent_cos
  Region:
  SecretId:
  SecretKey:
  Bucket:

servings:
  hosts:
    - 127.0.0.1:8000
fatemanager:
  host: 127.0.0.1
  port: 8001
  federatedId: 0
